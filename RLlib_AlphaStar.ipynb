{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6Ds2-amsbtr"
      },
      "outputs": [],
      "source": [
        "!pip install -q ray[RLlib]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSSCzk9dhyY2",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Borrow some boilerplate code from an earlier implementation. The interesting stuff is all in this notebook\n",
        "!git clone https://github.com/MatthewCWeston/RLlib_TicTacToe_League"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Code"
      ],
      "metadata": {
        "id": "7LM2QlBypURJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TicTacToe Env (Parallel)\n",
        "import gymnasium as gym\n",
        "from gymnasium.spaces import Box, Discrete\n",
        "\n",
        "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
        "from ray.rllib.utils.test_utils import (\n",
        "    add_rllib_example_script_args,\n",
        ")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "#from algo.constants import ACTION_MASK, OBSERVATIONS\n",
        "\n",
        "TTT_WIN_PATHS = np.array([\n",
        "    [0,1,2],[3,4,5],[6,7,8], # Horizontal\n",
        "    [0,3,6],[1,4,7],[2,5,8], # Vertical\n",
        "    [0,4,8],[2,4,6]          # Diagonal\n",
        "])\n",
        "\n",
        "class TicTacToe(MultiAgentEnv):\n",
        "    def __init__(self, config=None):\n",
        "        super().__init__()\n",
        "        self.agents = self.possible_agents = ['X','O']\n",
        "        #\"\"\"\n",
        "        space = gym.spaces.Dict({\n",
        "            ACTION_MASK: Box(0.0, 1.0, shape=(10,)), # Action 9 is NOP\n",
        "            OBSERVATIONS: Box(0.0, 1.0, (18,), np.float32)\n",
        "        })\n",
        "        self.observation_spaces = {\n",
        "            'X': space,\n",
        "            'O': space,\n",
        "        }\n",
        "        self.action_spaces = {\n",
        "            'X': Discrete(10),\n",
        "            'O': Discrete(10),\n",
        "        }\n",
        "        self.board = None\n",
        "        self.current_player = None\n",
        "    def get_obs(self):\n",
        "        board = self.board.copy()\n",
        "        am = np.zeros((10,))\n",
        "        am[:9] += 1-board.sum(axis=0)\n",
        "        am_inactive = np.zeros((10,))\n",
        "        am_inactive[9] = 1\n",
        "        obs = {\n",
        "            'X': {\n",
        "                OBSERVATIONS: board.flatten(),\n",
        "                ACTION_MASK: am_inactive,\n",
        "            },\n",
        "            'O': {\n",
        "                OBSERVATIONS: board[::-1].flatten(),\n",
        "                ACTION_MASK: am_inactive,\n",
        "            },\n",
        "        }\n",
        "        obs[self.current_player][ACTION_MASK] = am\n",
        "        return obs\n",
        "    def reset(self, *, seed=None, options=None):\n",
        "        self.board = np.zeros((2,9), dtype=np.float32)\n",
        "        self.current_player = 'X'\n",
        "        return self.get_obs(), {}\n",
        "    def step(self, action_dict):\n",
        "        action = action_dict[self.current_player]\n",
        "        rewards = {'X': 0.0, 'O': 0.0}\n",
        "        terminateds = {\"__all__\": False}\n",
        "        opponent = 'X' if self.current_player=='O' else 'O'\n",
        "        # The current player will always see himself as the first row and his opponent as the second row\n",
        "        board_ix = 0 if self.current_player=='X' else 1\n",
        "        if (action!=9): # Env checker ignores action mask; must check this.\n",
        "          self.board[board_ix][action] = 1\n",
        "        board = self.board[board_ix]\n",
        "        win_val = [1, 1, 1]\n",
        "        for p in TTT_WIN_PATHS:\n",
        "            if (board[p].sum()==3):\n",
        "                rewards[self.current_player] = 1.0\n",
        "                rewards[opponent] = -1.0\n",
        "                # Episode is done and needs to be reset for a new game.\n",
        "                terminateds[\"__all__\"] = True\n",
        "        # The board might also be full w/o any player having won/lost.\n",
        "        if (self.board.sum()==9) and (terminateds[\"__all__\"]==False):\n",
        "            terminateds[\"__all__\"] = True\n",
        "        #display_board(self.board)\n",
        "        self.current_player = opponent\n",
        "        obs = self.get_obs()\n",
        "        return (\n",
        "            obs,\n",
        "            rewards,\n",
        "            terminateds,\n",
        "            {},\n",
        "            {},\n",
        "        )\n",
        "\n",
        "def display_board(board):\n",
        "  board = board.reshape((2,9))\n",
        "  moves = []\n",
        "  for x, o in zip(board[0], board[1]):\n",
        "    if (x):\n",
        "      moves.append('X')\n",
        "    elif (o):\n",
        "      moves.append('O')\n",
        "    else:\n",
        "      moves.append('_')\n",
        "  print(moves[:3])\n",
        "  print(moves[3:6])\n",
        "  print(moves[6:])\n",
        "  print('-----')\n",
        "\n",
        "def convert_board(s):\n",
        "  s = s.replace('\\n','')\n",
        "  board = np.zeros((2,9))\n",
        "  for i, c in enumerate(s):\n",
        "    if (c=='X'):\n",
        "      board[0][i] = 1\n",
        "    elif (c=='O'):\n",
        "      board[1][i] = 1\n",
        "  return board\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "EhwN0L2IpWcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title eval_model\n",
        "def query_agent_2(obs_array, agent):\n",
        "  obs_array = obs_array.T\n",
        "  if (not isinstance(agent, torch.nn.Module)): # Heuristic, for testing\n",
        "    action = agent.forward_inference({Columns.OBS: torch.tensor(obs_array.flatten()).unsqueeze(0)})['actions'][0]\n",
        "    return int(action)\n",
        "  obs = {}\n",
        "  obs[OBSERVATIONS] = torch.tensor(obs_array).float().flatten()\n",
        "  am = torch.zeros(10)\n",
        "  am[:9] = torch.tensor(-obs_array.sum(axis=0)+1.).float().flatten()\n",
        "  obs[ACTION_MASK] = am\n",
        "  logits = agent.forward_inference({Columns.OBS: obs})['action_dist_inputs']\n",
        "  action = logits.argmax() # deterministic\n",
        "  return action.item()\n",
        "\n",
        "class BoardState():\n",
        "    '''\n",
        "      The 'opponent' is always [1], the agent is always [0].\n",
        "    '''\n",
        "    def __init__(self, state, prev=None, winner=-1):\n",
        "      self.state=state\n",
        "      self.prev=prev\n",
        "      self.winner=winner\n",
        "    def check_win(self, player):\n",
        "      b = self.state[:,player]\n",
        "      for p in TTT_WIN_PATHS:\n",
        "        if (b[p].sum()==3):\n",
        "          return True\n",
        "      return False\n",
        "    def propose(self, opponent_model):\n",
        "      next_states = []\n",
        "      occupied = self.state.sum(axis=1)\n",
        "      for i in range(9):\n",
        "        if occupied[i]==1:\n",
        "          continue\n",
        "        s = self.state.copy()\n",
        "        s[i][1] = 1\n",
        "        if (self.check_win(1)): # Did the model lose?\n",
        "          next_states.append(BoardState(s, prev=self, winner=1))\n",
        "          continue\n",
        "        if (s.sum() == 9):\n",
        "          continue # board is full\n",
        "        try:\n",
        "          act = query_agent_2(s, opponent_model)\n",
        "        except Exception as e:\n",
        "          print(s)\n",
        "          raise Exception()\n",
        "        s[act][0] = 1\n",
        "        if (self.check_win(0)): # Did the model win?\n",
        "          next_states.append(BoardState(s, prev=self, winner=0))\n",
        "          continue\n",
        "        next_states.append(BoardState(s, prev=self))\n",
        "      return next_states\n",
        "\n",
        "def eval_model(model):\n",
        "  winners = {0:[],1:[]}\n",
        "  model_first = np.zeros((9,2))\n",
        "  model_first[query_agent_2(model_first, model)][0] = 1\n",
        "  opponent_first = np.zeros((9,2))\n",
        "  boards_to_process = [BoardState(opponent_first), BoardState(model_first)]\n",
        "  max_sum = 0\n",
        "  while len(boards_to_process) > 0:\n",
        "    b = boards_to_process[0]\n",
        "    if (b.winner != -1):\n",
        "      winners[b.winner].append(b)\n",
        "    else:\n",
        "      boards_to_process.extend(b.propose(model))\n",
        "    del boards_to_process[0]\n",
        "\n",
        "  for i in range(2):\n",
        "    print(f'Winner: {i} count: {len(winners[i])}')\n",
        "  return [len(winners[i]) for i in [0,1]]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SLmp-WCXpkdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Code"
      ],
      "metadata": {
        "id": "8ARQ7bp_pVzA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj3EPT9cbaF4"
      },
      "source": [
        "### Initialize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oz8SH-7Yh1aU",
        "outputId": "1403fa26-3807-4178-9327-c27a796ae6e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/RLlib_TicTacToe_League\n"
          ]
        }
      ],
      "source": [
        "%cd RLlib_TicTacToe_League"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsH08y2dtuXr",
        "outputId": "f87bb021-1811-472e-e5da-dc2e07ae287f",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026-02-04 10:43:50,131\tWARNING ppo_torch_rl_module.py:8 -- DeprecationWarning: `ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module.PPOTorchRLModule` has been deprecated. Use `ray.rllib.algorithms.ppo.torch.default_ppo_torch_rl_module.DefaultPPOTorchRLModule` instead. This will raise an error in the future!\n"
          ]
        }
      ],
      "source": [
        "# @title Imports\n",
        "import numpy as np\n",
        "import functools\n",
        "\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "from ray.rllib.core.rl_module.multi_rl_module import MultiRLModuleSpec\n",
        "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
        "from ray.rllib.algorithms.ppo.torch.default_ppo_torch_rl_module import DefaultPPOTorchRLModule\n",
        "\n",
        "from ray.rllib.examples.rl_modules.classes.action_masking_rlm import (\n",
        "    ActionMaskingTorchRLModule,\n",
        ")\n",
        "\n",
        "from classes.heuristics import RandHeuristicRLM, BlockWinHeuristicRLM, PerfectHeuristicRLM\n",
        "from ray.rllib.utils.metrics import (\n",
        "    TRAINING_ITERATION_TIMER,\n",
        ")\n",
        "\n",
        "from ray.rllib.utils.metrics import ENV_RUNNER_RESULTS, EVALUATION_RESULTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBVogEKPVOJS"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Match Generation\n",
        "\n",
        "# Get the agent that will be learning this episode, from the set of learning agents\n",
        "def get_learning_agent(episode, policies_to_train):\n",
        "  # Returns the agent (X or O) and the ID of the 'student' policy\n",
        "  len_policies = len(policies_to_train)\n",
        "  eid = hash(episode.id_) % (2*len_policies)\n",
        "  agent_id = \"X\" if eid < len_policies else \"O\"\n",
        "  policy_id = policies_to_train[eid%len_policies]\n",
        "  return agent_id, policy_id\n",
        "\n",
        "def pfsp(agent, opponents, wr, rng):\n",
        "  if ('exploiter' not in agent):\n",
        "    # Main agents want to not-lose, and want to learn from opponents they lose to.\n",
        "    weights = np.array([wr[o][agent] for o in opponents])\n",
        "  else:\n",
        "    # Exploiter agents want to win, and want to learn from opponents they don't win against\n",
        "    weights = np.array([(1-wr[agent][o]) for o in opponents])\n",
        "  wr_sum = weights.sum()\n",
        "  if (wr_sum == 0):\n",
        "    return rng.choice(opponents)\n",
        "  return rng.choice(opponents, p=weights/wr_sum)\n",
        "\n",
        "def create_atm_fn(policies_to_train, agent_names, wr, just_added):\n",
        "  def atm_fn(agent_id, episode, **kwargs):\n",
        "    # The learning agent this episode is 'for', distributed evenly b/t X and O.\n",
        "    student_agent, student_policy = get_learning_agent(\n",
        "        episode,\n",
        "        policies_to_train\n",
        "    )\n",
        "    if (agent_id==student_agent):\n",
        "      return student_policy\n",
        "    eid = hash(episode.id_)\n",
        "    rng = np.random.default_rng(seed=abs(eid))\n",
        "    # Select an opponent.\n",
        "    if (student_policy==\"main\"): # opponents for main\n",
        "      rand = rng.random()\n",
        "      if (rand < .35): # 35% self play\n",
        "        return \"main\"\n",
        "      elif (rand < .85): # 50% PFSP (any other agent)\n",
        "        valid_options = filter(lambda s: s!='main', agent_names)\n",
        "      else: # 15% anything that beats main, SP otherwise\n",
        "        valid_options =list(filter(lambda s: wr[s]['main'] > wr['main'][s], agent_names))\n",
        "        if (len(valid_options)==0):\n",
        "          return \"main\"\n",
        "    elif (student_policy[:14] == \"main_exploiter\"): # opponents for ME\n",
        "      wr_thresh_me = wr[\"main\"][\"main_exploiter\"] / 9 # w/w+l >= 10%\n",
        "      if (wr[\"main_exploiter\"][\"main\"] > wr_thresh_me):\n",
        "        return \"main\" # play versus main, if it's doing passably\n",
        "      # Otherwise PFSP against main's past copies\n",
        "      valid_options = filter(lambda s: s[:6] == 'main_v', agent_names)\n",
        "    else: # opponents for LE (all past players; fig 1)\n",
        "      valid_options = filter(lambda s: '_v' in s, agent_names)\n",
        "    # Run PFSP on our options\n",
        "    valid_options = filter(lambda s: s not in just_added, valid_options)\n",
        "    return pfsp(student_policy, list(valid_options), wr, rng)\n",
        "  return atm_fn"
      ],
      "metadata": {
        "id": "AAzxyn_-YRtK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egpaa1FtkfhN",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title SelfPlayCallback (AlphaStar)\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from ray.rllib.callbacks.callbacks import RLlibCallback\n",
        "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
        "from ray.rllib.core.rl_module.multi_rl_module import MultiRLModuleSpec\n",
        "from ray.rllib.utils.metrics import ENV_RUNNER_RESULTS\n",
        "from ray.rllib.core import (\n",
        "    COMPONENT_RL_MODULE,\n",
        "    COMPONENT_LEARNER,\n",
        "    COMPONENT_LEARNER_GROUP,\n",
        ")\n",
        "\n",
        "def get_mc_string(agent1, agent2):\n",
        "  return '-'.join(sorted([agent1, agent2]))\n",
        "\n",
        "class SelfPlayCallback(RLlibCallback):\n",
        "    def __init__(self, clone_every=10):\n",
        "        super().__init__()\n",
        "        self.clone_every = clone_every\n",
        "        self.league = ['main','main_exploiter','league_exploiter','main_v0']\n",
        "        self.win_counts = defaultdict(lambda: defaultdict(lambda:0))\n",
        "        self.match_counts = defaultdict(lambda:0)\n",
        "        self.version_counter = defaultdict(lambda: 1)\n",
        "        # prior totals, stored on *learner*\n",
        "        self.win_counts_total = defaultdict(lambda: defaultdict(lambda:0))\n",
        "        self.match_counts_total = defaultdict(lambda:0)\n",
        "        # Hacky fix for newly added agents not working\n",
        "        self.just_added = []\n",
        "\n",
        "    def on_episode_end(\n",
        "        self,\n",
        "        *,\n",
        "        episode,\n",
        "        env_runner,\n",
        "        metrics_logger,\n",
        "        env,\n",
        "        env_index,\n",
        "        rl_module,\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "        x_agent = episode.module_for('X')\n",
        "        o_agent = episode.module_for('O')\n",
        "        # Avoid errors due to \"complete_episodes\" not being enabled\n",
        "        rewards = episode.get_rewards()\n",
        "        if ('X' not in rewards or 'O' not in rewards):\n",
        "          return\n",
        "        # Update stats\n",
        "        outcome = np.sign(rewards['X'][-1])\n",
        "        self.match_counts[get_mc_string(x_agent, o_agent)] += 1\n",
        "        metrics_logger.log_value(\n",
        "            f\"match_count_{get_mc_string(x_agent,o_agent)}\",\n",
        "            1.0,\n",
        "            reduce='lifetime_sum'\n",
        "        )\n",
        "        if (outcome != 0 and x_agent==o_agent):\n",
        "          metrics_logger.log_value(\n",
        "              f\"win_count_{x_agent}{x_agent}\",\n",
        "              0.5,\n",
        "              reduce='lifetime_sum'\n",
        "          )\n",
        "        elif (outcome == 1):\n",
        "          metrics_logger.log_value(\n",
        "              f\"win_count_{x_agent}{o_agent}\",\n",
        "              1.0,\n",
        "              reduce='lifetime_sum'\n",
        "          )\n",
        "        elif (outcome == -1):\n",
        "          metrics_logger.log_value(\n",
        "              f\"win_count_{o_agent}{x_agent}\",\n",
        "              1.0,\n",
        "              reduce='lifetime_sum'\n",
        "          )\n",
        "\n",
        "    def inherit_stats(self, new_module_id):\n",
        "        ''' Called on the envrunners; populates their stats for new agents. '''\n",
        "        to_clone = new_module_id.split('_v')[0]\n",
        "        # Inherit current agent's win statistics\n",
        "        for opponent in list(self.win_counts[to_clone].keys()):\n",
        "          if ('_v' not in opponent): # No reason to track b/t frozen agents\n",
        "            self.win_counts[new_module_id][opponent] = self.win_counts[to_clone][opponent]\n",
        "            self.win_counts[opponent][new_module_id] = self.win_counts[opponent][to_clone]\n",
        "            self.match_counts[get_mc_string(new_module_id, opponent)] = self.match_counts[get_mc_string(to_clone, opponent)]\n",
        "          # soft reset counts after cloning; fresh start\n",
        "          if (self.win_counts[to_clone][opponent] > 10):\n",
        "            self.match_counts[get_mc_string(to_clone, opponent)] /= 10.\n",
        "            self.win_counts[to_clone][opponent] /= 10.\n",
        "            if (to_clone!=opponent):\n",
        "              self.win_counts[opponent][to_clone] /= 10.\n",
        "\n",
        "    def clone_agent(self, algorithm, to_clone, reset_source=False):\n",
        "      # Clone an agent\n",
        "      vid = self.version_counter[to_clone]\n",
        "      self.version_counter[to_clone] += 1\n",
        "      new_module_id = f\"{to_clone}_v{vid}\"\n",
        "      self.just_added.append(new_module_id)\n",
        "      for opponent in self.league:\n",
        "        if (opponent==to_clone):\n",
        "          continue\n",
        "      self.league.append(new_module_id)\n",
        "      print(f\"adding new opponent to the mix ({new_module_id}).\")\n",
        "      self.inherit_stats(new_module_id)\n",
        "      cloned_module = algorithm.get_module(to_clone)\n",
        "      algorithm.add_module(\n",
        "          module_id=new_module_id,\n",
        "          module_spec=RLModuleSpec.from_module(cloned_module),\n",
        "      )\n",
        "      module_updates = {new_module_id: cloned_module.get_state(),}\n",
        "      # update shared critic, syncing weights (get_module only looks at envrunners, can't use)\n",
        "      sc = algorithm.learner_group._learner._module[SHARED_CRITIC_ID]\n",
        "      sc.new_agent_embedding(\n",
        "          name_to_mid(to_clone),\n",
        "          name_to_mid(new_module_id),\n",
        "      )\n",
        "      if (reset_source==True): # reset source agent weights to initial state\n",
        "        initial_agent = algorithm.learner_group._learner._module['main_v0']\n",
        "        module_updates[to_clone] = initial_agent.get_state()\n",
        "        sc.new_agent_embedding( # Reset its embedding, too\n",
        "            name_to_mid('main_v0'),\n",
        "            name_to_mid(to_clone),\n",
        "        )\n",
        "        self.win_counts[to_clone] = defaultdict(lambda:0)\n",
        "        self.match_counts[to_clone] = defaultdict(lambda:0)\n",
        "      # Share out updated shared critic to learners\n",
        "      module_updates[SHARED_CRITIC_ID] = sc.get_state()\n",
        "      # Syncs weights across everything (wait, why does it just say 'learner group'?)\n",
        "      algorithm.set_state(\n",
        "          {\n",
        "              COMPONENT_LEARNER_GROUP: {\n",
        "                  COMPONENT_LEARNER: {\n",
        "                      COMPONENT_RL_MODULE: module_updates\n",
        "                  }\n",
        "              },\n",
        "          }\n",
        "      )\n",
        "\n",
        "    def build_stats_from_results(self, result):\n",
        "        league_size = len(self.league)\n",
        "        wrs = defaultdict(lambda:defaultdict(lambda:0))\n",
        "        new_matches = {} # Just for debugging / observation\n",
        "        for i in range(league_size-1):\n",
        "          a = self.league[i]\n",
        "          for j in range(i, league_size):\n",
        "            b = self.league[j]\n",
        "            # Record statistics involving a learning agent\n",
        "            if (('_v' not in a) or ('_v' not in b)):\n",
        "              k = get_mc_string(a,b)\n",
        "              mc_new = result.get(f\"match_count_{k}\", 0)\n",
        "              wcab_new = result.get(f\"win_count_{a}{b}\", 0)\n",
        "              wcba_new = result.get(f\"win_count_{b}{a}\", 0)\n",
        "              new_matches[k] = mc_new - self.match_counts_total[k]\n",
        "              self.match_counts[k] += new_matches[k]\n",
        "              self.match_counts_total[k] = mc_new\n",
        "              self.win_counts[a][b] += wcab_new - self.win_counts_total[a][b]\n",
        "              self.win_counts_total[a][b] = wcab_new\n",
        "              if (a!=b): # Don't double-count wins for self-play\n",
        "                self.win_counts[b][a] += wcba_new - self.win_counts_total[b][a]\n",
        "                self.win_counts_total[b][a] = wcba_new\n",
        "              if (self.match_counts[k] != 0):\n",
        "                #'''# The actual rate of wins\n",
        "                # Useful for prioritizing threatening opponents, when using p=wr(b,a) rather than p=wr(a,b)*wr(b,a)\n",
        "                wrs[a][b] = self.win_counts[a][b] / self.match_counts[k]\n",
        "                wrs[b][a] = self.win_counts[b][a] / self.match_counts[k]\n",
        "              else:\n",
        "                wrs[a][b] = wrs[b][a] = 1/3 # Initialize as 33/33/33\n",
        "        return wrs, new_matches\n",
        "\n",
        "    def update_atm_fn(self, algorithm, wr):\n",
        "        # Set new mapping function\n",
        "        ptt = algorithm.config.policies_to_train.copy()\n",
        "        ptt.remove(SHARED_CRITIC_ID)\n",
        "        agent_to_module_mapping_fn = create_atm_fn(ptt, self.league, wr, self.just_added)\n",
        "        algorithm.config._is_frozen = False\n",
        "        algorithm.config.multi_agent(policy_mapping_fn=agent_to_module_mapping_fn)\n",
        "        algorithm.config.freeze()\n",
        "        # Add to (training) EnvRunners.\n",
        "        def _add(_env_runner, _module_spec=None):\n",
        "            _env_runner.config.multi_agent(\n",
        "                policy_mapping_fn=agent_to_module_mapping_fn,\n",
        "            )\n",
        "            return MultiRLModuleSpec.from_module(_env_runner.module)\n",
        "        algorithm.env_runner_group.foreach_env_runner(_add)\n",
        "\n",
        "    def on_train_result(self, *, algorithm, metrics_logger=None, result, **kwargs):\n",
        "        # Rebuild a set of stats with information from our env runners\n",
        "        wrs, nm = self.build_stats_from_results(result[ENV_RUNNER_RESULTS])\n",
        "        #\n",
        "        to_clone = set({})\n",
        "        reset_clone = defaultdict(lambda:False)\n",
        "        self.just_added = []\n",
        "        iter = algorithm.iteration\n",
        "        print(f\"Iter={iter}:\")\n",
        "        print(f\"Matchups: {dict(self.match_counts)}\")\n",
        "        print(f\"Win rates:\")\n",
        "        #for a, wrd in wrs.items():\n",
        "        for a in ['main', 'main_exploiter', 'league_exploiter']:\n",
        "          print(f'\\t{a}')\n",
        "          for o, wr in wrs[a].items():\n",
        "            new_matches = nm[get_mc_string(a,o)]\n",
        "            if (new_matches!=0):\n",
        "              wr_inv = wrs[o][a]\n",
        "              dw = 1 - wr - wr_inv\n",
        "              print(f'\\t\\t{o}: {wr:.02f}-{dw:.02f}-{wr_inv:.02f} (+{new_matches})')\n",
        "        if (iter)%self.clone_every==0:\n",
        "          to_clone.add('main')\n",
        "          to_clone.add('league_exploiter')\n",
        "          if (iter)%(2*self.clone_every)==0:\n",
        "            to_clone.add('main_exploiter')\n",
        "        # Special cloning criteria\n",
        "        # Main exploiter\n",
        "        clone_main_exploiter = True\n",
        "        for o in self.win_counts['main_exploiter'].keys():\n",
        "          if (o != 'main' and o[:6] != 'main_v'):\n",
        "            continue # Ignore non-main agents\n",
        "          if self.win_counts['main_exploiter'][o] < (1./0.7)*self.win_counts[o]['main_exploiter']:\n",
        "            clone_main_exploiter = False\n",
        "        if clone_main_exploiter:\n",
        "          to_clone.add('main_exploiter')\n",
        "          print('Cloning main exploiter on merit')\n",
        "        # League exploiter\n",
        "        le_wc = [self.win_counts['league_exploiter'][o] >= (1./0.7)*self.win_counts[o]['league_exploiter'] for o in self.win_counts['league_exploiter'].keys()]\n",
        "        if np.all(le_wc):\n",
        "          to_clone.add('league_exploiter')\n",
        "          reset_clone['league_exploiter'] = np.random.rand() < 0.25\n",
        "          print('Cloning league exploiter on merit')\n",
        "        # Clone agents\n",
        "        for c in to_clone:\n",
        "          self.clone_agent(algorithm, c, reset_source=reset_clone[c])\n",
        "        # Update mapping function, reweighting and adding new module if needed\n",
        "        self.update_atm_fn(algorithm, wrs)\n",
        "        # +2 = main + exploiter\n",
        "        result[\"league_size\"] = len(self.league)\n",
        "\n",
        "    @override(RLlibCallback)\n",
        "    def on_sample_end(\n",
        "        self,\n",
        "        *,\n",
        "        env_runner = None,\n",
        "        metrics_logger = None,\n",
        "        samples,\n",
        "        # TODO (sven): Deprecate these args.\n",
        "        worker = None,\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "        if (not env_runner.config.in_evaluation):\n",
        "          if (samples[0].env_t_started > 0):\n",
        "            del samples[0] # might be a continuation\n",
        "          if (not samples[-1].is_done):\n",
        "            del samples[-1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq4Z5Kc2lvAa"
      },
      "source": [
        "## Reinforcement Learning Arch\n",
        " - We add a shared critic to AlphaStar, and augment the observations of that critic with the IDs of both the viewpoint agent and the opposing agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzmxLAmombWJ"
      },
      "outputs": [],
      "source": [
        "SHARED_CRITIC_ID = \"shared_critic\"\n",
        "AUGMENTATION = \"augmentation\"\n",
        "\n",
        "ACTION_MASK = \"action_mask\"\n",
        "OBSERVATIONS = \"observations\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yqaakzx7VMNB",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title CMAPPOConfig\n",
        "import logging\n",
        "from typing import Any, Dict, List, Optional, Type, Union, TYPE_CHECKING\n",
        "\n",
        "from ray.rllib.algorithms.algorithm import Algorithm\n",
        "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig, NotProvided\n",
        "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
        "from ray.rllib.execution.rollout_ops import (\n",
        "    standardize_fields,\n",
        "    synchronous_parallel_sample,\n",
        ")\n",
        "from ray.rllib.execution.train_ops import (\n",
        "    train_one_step,\n",
        "    multi_gpu_train_one_step,\n",
        ")\n",
        "from ray.rllib.policy.policy import Policy\n",
        "from ray.rllib.utils.annotations import OldAPIStack, override\n",
        "from ray._common.deprecation import DEPRECATED_VALUE\n",
        "from ray.rllib.utils.metrics import (\n",
        "    ENV_RUNNER_RESULTS,\n",
        "    ENV_RUNNER_SAMPLING_TIMER,\n",
        "    LEARNER_RESULTS,\n",
        "    LEARNER_UPDATE_TIMER,\n",
        "    NUM_AGENT_STEPS_SAMPLED,\n",
        "    NUM_ENV_STEPS_SAMPLED,\n",
        "    NUM_ENV_STEPS_SAMPLED_LIFETIME,\n",
        "    SYNCH_WORKER_WEIGHTS_TIMER,\n",
        "    SAMPLE_TIMER,\n",
        "    TIMERS,\n",
        "    ALL_MODULES,\n",
        ")\n",
        "from ray.rllib.utils.metrics.learner_info import LEARNER_STATS_KEY\n",
        "from ray.rllib.utils.schedules.scheduler import Scheduler\n",
        "from ray.rllib.utils.typing import ResultDict\n",
        "from ray.util.debug import log_once\n",
        "\n",
        "from ray.rllib.algorithms.ppo.ppo import PPOConfig, PPO\n",
        "\n",
        "if TYPE_CHECKING:\n",
        "    from ray.rllib.core.learner.learner import Learner\n",
        "\n",
        "# Our code\n",
        "from algo.CMAPPOTorchLearner import CMAPPOTorchLearner\n",
        "from algo.modules.DefaultCMAPPOTorchRLModule import DefaultCMAPPOTorchRLModule\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "LEARNER_RESULTS_KL_KEY = \"mean_kl_loss\"\n",
        "LEARNER_RESULTS_CURR_KL_COEFF_KEY = \"curr_kl_coeff\"\n",
        "LEARNER_RESULTS_CURR_ENTROPY_COEFF_KEY = \"curr_entropy_coeff\"\n",
        "\n",
        "class CMAPPO(PPO):\n",
        "    @classmethod\n",
        "    @override(Algorithm)\n",
        "    def get_default_config(cls) -> AlgorithmConfig:\n",
        "        return CMAPPOConfig()\n",
        "\n",
        "\n",
        "class CMAPPOConfig(PPOConfig): # AlgorithmConfig -> PPOConfig -> CMAPPO\n",
        "    \"\"\"Defines a configuration class from which a CMAPPO Algorithm can be built.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, algo_class=None):\n",
        "        \"\"\"Initializes a CMAPPOConfig instance.\"\"\"\n",
        "        self.exploration_config = {\n",
        "            # The Exploration class to use. In the simplest case, this is the name\n",
        "            # (str) of any class present in the `rllib.utils.exploration` package.\n",
        "            # You can also provide the python class directly or the full location\n",
        "            # of your class (e.g. \"ray.rllib.utils.exploration.epsilon_greedy.\n",
        "            # EpsilonGreedy\").\n",
        "            \"type\": \"StochasticSampling\",\n",
        "            # Add constructor kwargs here (if any).\n",
        "        }\n",
        "\n",
        "        super().__init__(algo_class=algo_class or CMAPPO)\n",
        "\n",
        "        # fmt: off\n",
        "        # __sphinx_doc_begin__\n",
        "        self.lr = 5e-5\n",
        "        self.rollout_fragment_length = \"auto\"\n",
        "        self.train_batch_size = 4000\n",
        "\n",
        "        # PPO specific settings:\n",
        "        self.num_epochs = 30\n",
        "        self.minibatch_size = 128\n",
        "        self.shuffle_batch_per_epoch = True\n",
        "        self.lambda_ = 1.0\n",
        "        self.use_kl_loss = True\n",
        "        self.kl_coeff = 0.2\n",
        "        self.kl_target = 0.01\n",
        "        self.entropy_coeff = 0.0\n",
        "        self.clip_param = 0.3\n",
        "        self.grad_clip = None\n",
        "\n",
        "        # Override some of AlgorithmConfig's default values with PPO-specific values.\n",
        "        self.num_env_runners = 2\n",
        "        # __sphinx_doc_end__\n",
        "        # fmt: on\n",
        "\n",
        "        self.entropy_coeff_schedule = None  # OldAPIStack\n",
        "        self.lr_schedule = None  # OldAPIStack\n",
        "\n",
        "        # Deprecated keys.\n",
        "        self.sgd_minibatch_size = DEPRECATED_VALUE\n",
        "\n",
        "    @override(AlgorithmConfig)\n",
        "    def get_default_rl_module_spec(self) -> RLModuleSpec:\n",
        "        if self.framework_str == \"torch\":\n",
        "            return RLModuleSpec(module_class=DefaultCMAPPOTorchRLModule)\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @override(AlgorithmConfig)\n",
        "    def get_default_learner_class(self) -> Union[Type[\"Learner\"], str]:\n",
        "        if self.framework_str == \"torch\":\n",
        "            return CMAPPOTorchLearner\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @override(AlgorithmConfig)\n",
        "    def training(\n",
        "        self,\n",
        "        *,\n",
        "        lambda_: Optional[float] = NotProvided,\n",
        "        use_kl_loss: Optional[bool] = NotProvided,\n",
        "        kl_coeff: Optional[float] = NotProvided,\n",
        "        kl_target: Optional[float] = NotProvided,\n",
        "        entropy_coeff: Optional[float] = NotProvided,\n",
        "        entropy_coeff_schedule: Optional[List[List[Union[int, float]]]] = NotProvided,\n",
        "        clip_param: Optional[float] = NotProvided,\n",
        "        grad_clip: Optional[float] = NotProvided,\n",
        "        # OldAPIStack\n",
        "        lr_schedule: Optional[List[List[Union[int, float]]]] = NotProvided,\n",
        "        **kwargs,\n",
        "    ) -> \"PPOConfig\":\n",
        "        # Pass kwargs onto super's `training()` method.\n",
        "        super().training(**kwargs)\n",
        "        if lambda_ is not NotProvided:\n",
        "            self.lambda_ = lambda_\n",
        "        if use_kl_loss is not NotProvided:\n",
        "            self.use_kl_loss = use_kl_loss\n",
        "        if kl_coeff is not NotProvided:\n",
        "            self.kl_coeff = kl_coeff\n",
        "        if kl_target is not NotProvided:\n",
        "            self.kl_target = kl_target\n",
        "        if entropy_coeff is not NotProvided:\n",
        "            self.entropy_coeff = entropy_coeff\n",
        "        if clip_param is not NotProvided:\n",
        "            self.clip_param = clip_param\n",
        "        if grad_clip is not NotProvided:\n",
        "            self.grad_clip = grad_clip\n",
        "\n",
        "        # TODO (sven): Remove these once new API stack is only option for PPO.\n",
        "        if lr_schedule is not NotProvided:\n",
        "            self.lr_schedule = lr_schedule\n",
        "        if entropy_coeff_schedule is not NotProvided:\n",
        "            self.entropy_coeff_schedule = entropy_coeff_schedule\n",
        "\n",
        "        return self\n",
        "\n",
        "    @override(AlgorithmConfig)\n",
        "    def validate(self) -> None:\n",
        "        # Call super's validation method.\n",
        "        super().validate()\n",
        "\n",
        "        # Synchronous sampling, on-policy/PPO algos -> Check mismatches between\n",
        "        # `rollout_fragment_length` and `train_batch_size_per_learner` to avoid user\n",
        "        # confusion.\n",
        "        # TODO (sven): Make rollout_fragment_length a property and create a private\n",
        "        #  attribute to store (possibly) user provided value (or \"auto\") in. Deprecate\n",
        "        #  `self.get_rollout_fragment_length()`.\n",
        "        self.validate_train_batch_size_vs_rollout_fragment_length()\n",
        "\n",
        "        # SGD minibatch size must be smaller than train_batch_size (b/c\n",
        "        # we subsample a batch of `minibatch_size` from the train-batch for\n",
        "        # each `num_epochs`).\n",
        "        if (\n",
        "            not self.enable_rl_module_and_learner\n",
        "            and self.minibatch_size > self.train_batch_size\n",
        "        ):\n",
        "            self._value_error(\n",
        "                f\"`minibatch_size` ({self.minibatch_size}) must be <= \"\n",
        "                f\"`train_batch_size` ({self.train_batch_size}). In PPO, the train batch\"\n",
        "                f\" will be split into {self.minibatch_size} chunks, each of which \"\n",
        "                f\"is iterated over (used for updating the policy) {self.num_epochs} \"\n",
        "                \"times.\"\n",
        "            )\n",
        "        elif self.enable_rl_module_and_learner:\n",
        "            mbs = self.minibatch_size\n",
        "            tbs = self.train_batch_size_per_learner or self.train_batch_size\n",
        "            if isinstance(mbs, int) and isinstance(tbs, int) and mbs > tbs:\n",
        "                self._value_error(\n",
        "                    f\"`minibatch_size` ({mbs}) must be <= \"\n",
        "                    f\"`train_batch_size_per_learner` ({tbs}). In PPO, the train batch\"\n",
        "                    f\" will be split into {mbs} chunks, each of which is iterated over \"\n",
        "                    f\"(used for updating the policy) {self.num_epochs} times.\"\n",
        "                )\n",
        "\n",
        "        # Episodes may only be truncated (and passed into PPO's\n",
        "        # `postprocessing_fn`), iff generalized advantage estimation is used\n",
        "        # (value function estimate at end of truncated episode to estimate\n",
        "        # remaining value).\n",
        "        if (\n",
        "            not self.in_evaluation\n",
        "            and self.batch_mode == \"truncate_episodes\"\n",
        "            and not self.use_gae\n",
        "        ):\n",
        "            self._value_error(\n",
        "                \"Episode truncation is not supported without a value \"\n",
        "                \"function (to estimate the return at the end of the truncated\"\n",
        "                \" trajectory). Consider setting \"\n",
        "                \"batch_mode=complete_episodes.\"\n",
        "            )\n",
        "\n",
        "        # New API stack checks.\n",
        "        if self.enable_rl_module_and_learner:\n",
        "            # `lr_schedule` checking.\n",
        "            if self.lr_schedule is not None:\n",
        "                self._value_error(\n",
        "                    \"`lr_schedule` is deprecated and must be None! Use the \"\n",
        "                    \"`lr` setting to setup a schedule.\"\n",
        "                )\n",
        "            if self.entropy_coeff_schedule is not None:\n",
        "                self._value_error(\n",
        "                    \"`entropy_coeff_schedule` is deprecated and must be None! Use the \"\n",
        "                    \"`entropy_coeff` setting to setup a schedule.\"\n",
        "                )\n",
        "            Scheduler.validate(\n",
        "                fixed_value_or_schedule=self.entropy_coeff,\n",
        "                setting_name=\"entropy_coeff\",\n",
        "                description=\"entropy coefficient\",\n",
        "            )\n",
        "        if isinstance(self.entropy_coeff, float) and self.entropy_coeff < 0.0:\n",
        "            self._value_error(\"`entropy_coeff` must be >= 0.0\")\n",
        "\n",
        "    @property\n",
        "    @override(AlgorithmConfig)\n",
        "    def _model_config_auto_includes(self) -> Dict[str, Any]:\n",
        "        return super()._model_config_auto_includes | {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LK1EhI1ll3W5"
      },
      "outputs": [],
      "source": [
        "# @title CMAPPOLearner\n",
        "import abc\n",
        "from typing import Any, Dict\n",
        "\n",
        "from ray.rllib.algorithms.ppo.ppo import (\n",
        "    LEARNER_RESULTS_CURR_ENTROPY_COEFF_KEY,\n",
        "    LEARNER_RESULTS_KL_KEY,\n",
        "    PPOConfig,\n",
        ")\n",
        "from ray.rllib.connectors.learner import (\n",
        "    AddOneTsToEpisodesAndTruncate,\n",
        "    GeneralAdvantageEstimation,\n",
        ")\n",
        "from ray.rllib.core.learner.learner import Learner\n",
        "from ray.rllib.core.rl_module.apis.value_function_api import ValueFunctionAPI\n",
        "from ray.rllib.utils.annotations import (\n",
        "    override,\n",
        "    OverrideToImplementCustomLogic_CallToSuperRecommended,\n",
        ")\n",
        "from ray.rllib.utils.lambda_defaultdict import LambdaDefaultDict\n",
        "from ray.rllib.utils.metrics import (\n",
        "    NUM_ENV_STEPS_SAMPLED_LIFETIME,\n",
        "    NUM_MODULE_STEPS_TRAINED,\n",
        ")\n",
        "from ray.rllib.utils.numpy import convert_to_numpy\n",
        "from ray.rllib.utils.schedules.scheduler import Scheduler\n",
        "from ray.rllib.utils.typing import ModuleID, TensorType\n",
        "\n",
        "\n",
        "class CMAPPOLearner(Learner):\n",
        "\n",
        "    # Deal with GAE somehow. Maybe skip it and move the logic over here, into the value loss calculation method. Saves us a VF pass, too.\n",
        "    @override(Learner)\n",
        "    def build(self) -> None:\n",
        "        super().build() # We call Learner's build function, not PPOLearner's\n",
        "        learner_config_dict = self.config.learner_config_dict\n",
        "\n",
        "        # Dict mapping module IDs to the respective entropy Scheduler instance.\n",
        "        self.entropy_coeff_schedulers_per_module: Dict[\n",
        "            ModuleID, Scheduler\n",
        "        ] = LambdaDefaultDict(\n",
        "            lambda module_id: Scheduler(\n",
        "                fixed_value_or_schedule=(\n",
        "                    self.config.get_config_for_module(module_id).entropy_coeff\n",
        "                ),\n",
        "                framework=self.framework,\n",
        "                device=self._device,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Set up KL coefficient variables (per module).\n",
        "        # Note that the KL coeff is not controlled by a Scheduler, but seeks\n",
        "        # to stay close to a given kl_target value.\n",
        "        self.curr_kl_coeffs_per_module: Dict[ModuleID, TensorType] = LambdaDefaultDict(\n",
        "            lambda module_id: self._get_tensor_variable(\n",
        "                self.config.get_config_for_module(module_id).kl_coeff\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Extend all episodes by one artificial timestep to allow the value function net\n",
        "        # to compute the bootstrap values (and add a mask to the batch to know, which\n",
        "        # slots to mask out).\n",
        "        if (\n",
        "            self._learner_connector is not None\n",
        "            and self.config.add_default_connectors_to_learner_pipeline\n",
        "        ):\n",
        "            # At the end of the pipeline (when the batch is already completed), add the\n",
        "            # GAE connector, which performs a vf forward pass, then computes the GAE\n",
        "            # computations, and puts the results of this (advantages, value targets)\n",
        "            # directly back in the batch. This is then the batch used for\n",
        "            # `forward_train` and `compute_losses`.\n",
        "            self._learner_connector.append(\n",
        "                CMAPPOGAEConnector(\n",
        "                    gamma=self.config.gamma,\n",
        "                    lambda_=self.config.lambda_,\n",
        "                    policies_to_train=learner_config_dict['policies_to_train']\n",
        "                )\n",
        "            )\n",
        "\n",
        "    @override(Learner)\n",
        "    def remove_module(self, module_id: ModuleID, **kwargs):\n",
        "        marl_spec = super().remove_module(module_id, **kwargs)\n",
        "        if (module_id != SHARED_CRITIC_ID):\n",
        "          self.entropy_coeff_schedulers_per_module.pop(module_id, None)\n",
        "          self.curr_kl_coeffs_per_module.pop(module_id, None)\n",
        "        return marl_spec\n",
        "\n",
        "    @OverrideToImplementCustomLogic_CallToSuperRecommended\n",
        "    @override(Learner)\n",
        "    def after_gradient_based_update(\n",
        "        self,\n",
        "        *,\n",
        "        timesteps: Dict[str, Any],\n",
        "    ) -> None:\n",
        "        super().after_gradient_based_update(timesteps=timesteps)\n",
        "\n",
        "        for module_id, module in self.module._rl_modules.items():\n",
        "            if (module_id == SHARED_CRITIC_ID):\n",
        "              continue # Policy terms irrelevant to shared critic.\n",
        "            config = self.config.get_config_for_module(module_id)\n",
        "            # Update entropy coefficient via our Scheduler.\n",
        "            new_entropy_coeff = self.entropy_coeff_schedulers_per_module[\n",
        "                module_id\n",
        "            ].update(timestep=timesteps.get(NUM_ENV_STEPS_SAMPLED_LIFETIME, 0))\n",
        "            self.metrics.log_value(\n",
        "                (module_id, LEARNER_RESULTS_CURR_ENTROPY_COEFF_KEY),\n",
        "                new_entropy_coeff,\n",
        "                window=1,\n",
        "            )\n",
        "            if (\n",
        "                config.use_kl_loss\n",
        "                and self.metrics.peek((module_id, NUM_MODULE_STEPS_TRAINED), default=0)\n",
        "                > 0\n",
        "                and (module_id, LEARNER_RESULTS_KL_KEY) in self.metrics\n",
        "            ):\n",
        "                kl_loss = convert_to_numpy(\n",
        "                    self.metrics.peek((module_id, LEARNER_RESULTS_KL_KEY))\n",
        "                )\n",
        "                self._update_module_kl_coeff(\n",
        "                    module_id=module_id,\n",
        "                    config=config,\n",
        "                    kl_loss=kl_loss,\n",
        "                )\n",
        "\n",
        "    @classmethod\n",
        "    @override(Learner)\n",
        "    def rl_module_required_apis(cls) -> list[type]:\n",
        "        # We no longer require value functions for modules, since there's a central critic\n",
        "        return []\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def _update_module_kl_coeff(\n",
        "        self,\n",
        "        *,\n",
        "        module_id: ModuleID,\n",
        "        config: PPOConfig,\n",
        "        kl_loss: float,\n",
        "    ) -> None:\n",
        "        \"\"\"Dynamically update the KL loss coefficients of each module.\n",
        "\n",
        "        The update is completed using the mean KL divergence between the action\n",
        "        distributions current policy and old policy of each module. That action\n",
        "        distribution is computed during the most recent update/call to `compute_loss`.\n",
        "\n",
        "        Args:\n",
        "            module_id: The module whose KL loss coefficient to update.\n",
        "            config: The AlgorithmConfig specific to the given `module_id`.\n",
        "            kl_loss: The mean KL loss of the module, computed inside\n",
        "                `compute_loss_for_module()`.\n",
        "        \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FoOPoyH5MXi"
      },
      "source": [
        "## Shared critic module"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title CMAPPOTorchLearner\n",
        "import logging\n",
        "from typing import Any, Dict\n",
        "from collections.abc import Callable\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from ray.rllib.algorithms.ppo.ppo import (\n",
        "    LEARNER_RESULTS_KL_KEY,\n",
        "    LEARNER_RESULTS_CURR_KL_COEFF_KEY,\n",
        "    LEARNER_RESULTS_VF_EXPLAINED_VAR_KEY,\n",
        "    LEARNER_RESULTS_VF_LOSS_UNCLIPPED_KEY,\n",
        "    PPOConfig,\n",
        ")\n",
        "from ray.rllib.core.columns import Columns\n",
        "from ray.rllib.core.learner.learner import Learner, POLICY_LOSS_KEY, VF_LOSS_KEY, ENTROPY_KEY\n",
        "from ray.rllib.core.learner.torch.torch_learner import TorchLearner\n",
        "from ray.rllib.evaluation.postprocessing import Postprocessing\n",
        "from ray.rllib.utils.annotations import override\n",
        "from ray.rllib.utils.framework import try_import_torch\n",
        "from ray.rllib.utils.torch_utils import explained_variance\n",
        "from ray.rllib.utils.typing import ModuleID, TensorType\n",
        "\n",
        "# From Learner\n",
        "from ray.rllib.core.rl_module.apis import SelfSupervisedLossAPI\n",
        "\n",
        "torch, nn = try_import_torch()\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class CMAPPOTorchLearner(CMAPPOLearner, TorchLearner):\n",
        "    def get_pmm(\n",
        "            self,\n",
        "            batch: Dict[str, Any],\n",
        "            is_actor: bool = False,\n",
        "        ) -> Callable:\n",
        "        \"\"\" Gets the possibly_masked_mean function \"\"\"\n",
        "        mask = None\n",
        "        if (is_actor and TEACHER_MASK in batch): # no actor update if teaching\n",
        "            if Columns.LOSS_MASK in batch:\n",
        "                mask = batch[Columns.LOSS_MASK] * batch[TEACHER_MASK]\n",
        "            else:\n",
        "                mask = batch[TEACHER_MASK]\n",
        "        elif Columns.LOSS_MASK in batch:\n",
        "            mask = batch[Columns.LOSS_MASK]\n",
        "        if (mask is not None):\n",
        "            return lambda data_: torch.sum(data_[mask]) / torch.sum(mask)\n",
        "        return torch.mean\n",
        "\n",
        "    \"\"\"\n",
        "      Implements MAPPO in Torch, on top of a MAPPOLearner.\n",
        "    \"\"\"\n",
        "    def compute_loss_for_critic(\n",
        "        self,\n",
        "        batch: Dict[str, Any]\n",
        "    ):\n",
        "      \"\"\"\n",
        "        Computes loss for critic, and returns a list of advantages and rewards for the target batch.\n",
        "      \"\"\"\n",
        "      possibly_masked_mean = self.get_pmm(batch)\n",
        "      module = self.module[SHARED_CRITIC_ID].unwrapped()\n",
        "      vf_preds = module.compute_values(batch)\n",
        "      vf_targets = batch[Postprocessing.VALUE_TARGETS]\n",
        "      # Compute a value function loss.\n",
        "      vf_loss = torch.pow(vf_preds - vf_targets, 2.0)\n",
        "      vf_loss_clipped = torch.clamp(vf_loss, 0, self.config.vf_clip_param)\n",
        "      mean_vf_loss = possibly_masked_mean(vf_loss_clipped)\n",
        "      mean_vf_unclipped_loss = possibly_masked_mean(vf_loss)\n",
        "      # record metrics\n",
        "      self.metrics.log_dict(\n",
        "          {\n",
        "              VF_LOSS_KEY: mean_vf_loss,\n",
        "              LEARNER_RESULTS_VF_LOSS_UNCLIPPED_KEY: mean_vf_unclipped_loss,\n",
        "              LEARNER_RESULTS_VF_EXPLAINED_VAR_KEY: explained_variance(\n",
        "                  vf_targets, vf_preds\n",
        "              ),\n",
        "          },\n",
        "          key=SHARED_CRITIC_ID,\n",
        "          window=1,\n",
        "      )\n",
        "      return mean_vf_loss\n",
        "\n",
        "    # Apply central critic logic here. Compute advantages and losses for critic first, then use the results to calculate policy loss.\n",
        "    @override(Learner)\n",
        "    def compute_losses(\n",
        "        self, *, fwd_out: Dict[str, Any], batch: Dict[str, Any]\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            fwd_out: Output from a call to the `forward_train()` method of the\n",
        "                underlying MultiRLModule (`self.module`) during training\n",
        "                (`self.update()`).\n",
        "            batch: The train batch that was used to compute `fwd_out`.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary mapping module IDs to individual loss terms.\n",
        "        \"\"\"\n",
        "        loss_per_module = {SHARED_CRITIC_ID: 0}\n",
        "        # Calculate loss for agent policies\n",
        "        for module_id in fwd_out:\n",
        "            if (module_id == SHARED_CRITIC_ID): # Already computed\n",
        "              continue\n",
        "            #\n",
        "            module = self.module[module_id].unwrapped()\n",
        "            if isinstance(module, SelfSupervisedLossAPI):\n",
        "                # For e.g. enabling intrinsic curiosity modules.\n",
        "                loss = module.compute_self_supervised_loss(\n",
        "                    learner=self,\n",
        "                    module_id=module_id,\n",
        "                    config=self.config.get_config_for_module(module_id),\n",
        "                    batch=module_batch,\n",
        "                    fwd_out=module_fwd_out,\n",
        "                )\n",
        "            else:\n",
        "                module_batch = batch[module_id]\n",
        "                module_fwd_out = fwd_out[module_id]\n",
        "                # For every module we're going to touch, sans the critic\n",
        "                loss = self.compute_loss_for_module(\n",
        "                    module_id=module_id,\n",
        "                    config=self.config.get_config_for_module(module_id),\n",
        "                    batch=module_batch,\n",
        "                    fwd_out=module_fwd_out,\n",
        "                )\n",
        "                # Optimize the critic\n",
        "                loss_per_module[SHARED_CRITIC_ID] += self.compute_loss_for_critic(module_batch)\n",
        "            loss_per_module[module_id] = loss\n",
        "        #\n",
        "        return loss_per_module\n",
        "\n",
        "    # We strip out the value function optimization here.\n",
        "    @override(TorchLearner)\n",
        "    def compute_loss_for_module(\n",
        "        self,\n",
        "        *,\n",
        "        module_id: ModuleID,\n",
        "        config: PPOConfig,\n",
        "        batch: Dict[str, Any],\n",
        "        fwd_out: Dict[str, TensorType],\n",
        "    ) -> TensorType:\n",
        "        module = self.module[module_id].unwrapped()\n",
        "        possibly_masked_mean = self.get_pmm(batch, is_actor=True)\n",
        "        # Possibly apply masking to some sub loss terms and to the total loss term\n",
        "        # at the end. Masking could be used for RNN-based model (zero padded `batch`)\n",
        "        # and for PPO's batched value function (and bootstrap value) computations,\n",
        "        # for which we add an (artificial) timestep to each episode to\n",
        "        # simplify the actual computation.\n",
        "        action_dist_class_train = module.get_train_action_dist_cls()\n",
        "        action_dist_class_exploration = module.get_exploration_action_dist_cls()\n",
        "\n",
        "        curr_action_dist = action_dist_class_train.from_logits(\n",
        "            fwd_out[Columns.ACTION_DIST_INPUTS]\n",
        "        )\n",
        "        prev_action_dist = action_dist_class_exploration.from_logits(\n",
        "            batch[Columns.ACTION_DIST_INPUTS]\n",
        "        )\n",
        "\n",
        "        logp_ratio = torch.exp(\n",
        "            curr_action_dist.logp(batch[Columns.ACTIONS]) - batch[Columns.ACTION_LOGP]\n",
        "        )\n",
        "\n",
        "        # Only calculate kl loss if necessary (kl-coeff > 0.0).\n",
        "        if config.use_kl_loss:\n",
        "            action_kl = prev_action_dist.kl(curr_action_dist)\n",
        "            mean_kl_loss = possibly_masked_mean(action_kl)\n",
        "        else:\n",
        "            mean_kl_loss = torch.tensor(0.0, device=logp_ratio.device)\n",
        "\n",
        "        curr_entropy = curr_action_dist.entropy()\n",
        "        mean_entropy = possibly_masked_mean(curr_entropy)\n",
        "\n",
        "        surrogate_loss = torch.min(\n",
        "            batch[Postprocessing.ADVANTAGES] * logp_ratio,\n",
        "            batch[Postprocessing.ADVANTAGES]\n",
        "            * torch.clamp(logp_ratio, 1 - config.clip_param, 1 + config.clip_param),\n",
        "        )\n",
        "        # Remove critic loss from per-module computation\n",
        "        total_loss = possibly_masked_mean(\n",
        "            -surrogate_loss\n",
        "            - (\n",
        "                self.entropy_coeff_schedulers_per_module[module_id].get_current_value()\n",
        "                * curr_entropy\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Add mean_kl_loss (already processed through `possibly_masked_mean`),\n",
        "        # if necessary.\n",
        "        if config.use_kl_loss:\n",
        "            total_loss += self.curr_kl_coeffs_per_module[module_id] * mean_kl_loss\n",
        "\n",
        "        # Log important loss stats.\n",
        "        self.metrics.log_dict(\n",
        "            {\n",
        "                POLICY_LOSS_KEY: -possibly_masked_mean(surrogate_loss),\n",
        "                ENTROPY_KEY: mean_entropy,\n",
        "                LEARNER_RESULTS_KL_KEY: mean_kl_loss,\n",
        "            },\n",
        "            key=module_id,\n",
        "            window=1,  # <- single items (should not be mean/ema-reduced over time).\n",
        "        )\n",
        "        # Return the total loss.\n",
        "        return total_loss\n",
        "\n",
        "    # Untouched, leave it be.\n",
        "    @override(CMAPPOLearner)\n",
        "    def _update_module_kl_coeff(\n",
        "        self,\n",
        "        *,\n",
        "        module_id: ModuleID,\n",
        "        config: PPOConfig,\n",
        "        kl_loss: float,\n",
        "    ) -> None:\n",
        "        if np.isnan(kl_loss):\n",
        "            logger.warning(\n",
        "                f\"KL divergence for Module {module_id} is non-finite, this \"\n",
        "                \"will likely destabilize your model and the training \"\n",
        "                \"process. Action(s) in a specific state have near-zero \"\n",
        "                \"probability. This can happen naturally in deterministic \"\n",
        "                \"environments where the optimal policy has zero mass for a \"\n",
        "                \"specific action. To fix this issue, consider setting \"\n",
        "                \"`kl_coeff` to 0.0 or increasing `entropy_coeff` in your \"\n",
        "                \"config.\"\n",
        "            )\n",
        "\n",
        "        # Update the KL coefficient.\n",
        "        curr_var = self.curr_kl_coeffs_per_module[module_id]\n",
        "        if kl_loss > 2.0 * config.kl_target:\n",
        "            # TODO (Kourosh) why not 2?\n",
        "            curr_var.data *= 1.5\n",
        "        elif kl_loss < 0.5 * config.kl_target:\n",
        "            curr_var.data *= 0.5\n",
        "\n",
        "        # Log the updated KL-coeff value.\n",
        "        self.metrics.log_value(\n",
        "            (module_id, LEARNER_RESULTS_CURR_KL_COEFF_KEY),\n",
        "            curr_var.item(),\n",
        "            window=1,\n",
        "        )"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iFpChSpkdzG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title CMAPPOGAEConnector\n",
        "from typing import Any, List, Dict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "\n",
        "from ray.rllib.connectors.connector_v2 import ConnectorV2\n",
        "from ray.rllib.connectors.common.numpy_to_tensor import NumpyToTensor\n",
        "from ray.rllib.core.columns import Columns\n",
        "from ray.rllib.core.rl_module.apis.value_function_api import ValueFunctionAPI\n",
        "from ray.rllib.core.rl_module.multi_rl_module import MultiRLModule\n",
        "from ray.rllib.core.rl_module.torch import TorchRLModule\n",
        "from ray.rllib.evaluation.postprocessing import Postprocessing\n",
        "from ray.rllib.utils.annotations import override\n",
        "from ray.rllib.utils.numpy import convert_to_numpy\n",
        "from ray.rllib.utils.postprocessing.value_predictions import compute_value_targets\n",
        "from ray.rllib.utils.postprocessing.zero_padding import (\n",
        "    split_and_zero_pad_n_episodes,\n",
        "    unpad_data_if_necessary,\n",
        ")\n",
        "from ray.rllib.utils.typing import EpisodeType\n",
        "\n",
        "\n",
        "TEACHER_MASK = 'TEACHER_MASK'\n",
        "\n",
        "def name_to_mid(name):\n",
        "  '''\n",
        "      - 'main' has IDs 0 through 99\n",
        "      - 'league_exploiter' has IDs 100 through 199\n",
        "      - 'main_exploiter' has IDs 200 through 299\n",
        "  '''\n",
        "  name = name.split('_v')\n",
        "  version = 0 if len(name)==1 else int(name[-1])+1\n",
        "  assert version<100\n",
        "  base = {\n",
        "      'main':0,\n",
        "      'league_exploiter':100,\n",
        "      'main_exploiter':200,\n",
        "\n",
        "  }[name[0]]\n",
        "  return base+version\n",
        "\n",
        "class CMAPPOGAEConnector(ConnectorV2):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_observation_space=None,\n",
        "        input_action_space=None,\n",
        "        *,\n",
        "        gamma,\n",
        "        lambda_,\n",
        "        policies_to_train\n",
        "    ):\n",
        "        super().__init__(input_observation_space, input_action_space)\n",
        "        self.gamma = gamma\n",
        "        self.lambda_ = lambda_\n",
        "        self._numpy_to_tensor_connector = None\n",
        "        self.policies_to_train=policies_to_train\n",
        "\n",
        "    def augment_critic_identity(self, batch, meps):\n",
        "      '''\n",
        "          Provides the critic with a one-hot vector indicating the opponent's identity.\n",
        "\n",
        "          I think this code should also work for parallel actions. Gratuitious, but not incorrect.\n",
        "      '''\n",
        "      for aid in batch:\n",
        "        # Initialize next two augmentation IDs\n",
        "        b_obs = batch[aid][Columns.OBS]\n",
        "        batch[aid][AUGMENTATION] = torch.zeros((b_obs.shape[0],2), dtype=torch.long).to(b_obs.device)\n",
        "      start_indices = defaultdict(lambda: 0) # where to start in each agent's batch, when populating agent IDs for critic\n",
        "      lc = 0\n",
        "      for mep in meps:\n",
        "        x_ep, o_ep = mep.agent_episodes['X'], mep.agent_episodes['O']\n",
        "        x_mid, o_mid = x_ep.module_id, o_ep.module_id\n",
        "        x_l, o_l = len(x_ep), len(o_ep)\n",
        "        # Start indices. We alternate because we might have the same module.\n",
        "        x_s = start_indices[x_mid]\n",
        "        start_indices[x_mid]+=x_l\n",
        "        o_s = start_indices[o_mid]\n",
        "        start_indices[o_mid]+=o_l\n",
        "        x_mid_id, o_mid_id = name_to_mid(x_mid), name_to_mid(o_mid)\n",
        "        # Provide the IDs of the X and O agents\n",
        "        batch[x_mid][AUGMENTATION][x_s:x_s+x_l, 0] = x_mid_id\n",
        "        batch[x_mid][AUGMENTATION][x_s:x_s+x_l, 1] = o_mid_id\n",
        "        # Note that we've changed this such that the observer agent always has itself first.\n",
        "        batch[o_mid][AUGMENTATION][o_s:o_s+o_l, 0] = o_mid_id\n",
        "        batch[o_mid][AUGMENTATION][o_s:o_s+o_l, 1] = x_mid_id\n",
        "\n",
        "    def mask_teacher_batches(self, meps, batch):\n",
        "      for aid in batch:\n",
        "        b_obs = batch[aid][Columns.OBS]\n",
        "        if (TEACHER_MASK not in batch[aid].keys()):\n",
        "          batch[aid][TEACHER_MASK] = torch.ones((b_obs.shape[0],), dtype=torch.bool).to(b_obs.device)\n",
        "      start_indices = defaultdict(lambda: 0)\n",
        "      lc = 0\n",
        "      for mep in meps:\n",
        "        student_agent, student_policy = get_learning_agent(mep, self.policies_to_train)\n",
        "        x_ep, o_ep = mep.agent_episodes['X'], mep.agent_episodes['O']\n",
        "        x_mid, o_mid = x_ep.module_id, o_ep.module_id\n",
        "        x_l, o_l = len(x_ep), len(o_ep)\n",
        "        x_s = start_indices[x_mid]\n",
        "        start_indices[x_mid]+=x_l\n",
        "        o_s = start_indices[o_mid]\n",
        "        start_indices[o_mid]+=o_l\n",
        "        if (x_mid!=student_policy):\n",
        "          batch[x_mid][TEACHER_MASK][x_s:x_s+x_l] = False\n",
        "        elif (o_mid!=student_policy):\n",
        "          batch[o_mid][TEACHER_MASK][o_s:o_s+o_l] = False\n",
        "\n",
        "    @override(ConnectorV2)\n",
        "    def __call__(\n",
        "        self,\n",
        "        *,\n",
        "        rl_module: MultiRLModule,\n",
        "        episodes: List[EpisodeType],\n",
        "        batch: Dict[str, Any],\n",
        "        **kwargs,\n",
        "    ):\n",
        "        # Device to place all GAE result tensors (advantages and value targets) on.\n",
        "        device = None\n",
        "\n",
        "        # Extract all single-agent episodes.\n",
        "        sa_episodes_list = list(\n",
        "            self.single_agent_episode_iterator(episodes, agents_that_stepped_only=False)\n",
        "        )\n",
        "        # Perform the value nets' forward passes.\n",
        "        vf_preds = rl_module.foreach_module(\n",
        "            func=lambda mid, module: (\n",
        "                module.compute_values(batch[mid])\n",
        "                if mid in batch and isinstance(module, ValueFunctionAPI)\n",
        "                else None\n",
        "            ),\n",
        "            return_dict=True,\n",
        "        )\n",
        "        # Mask the agents that should be learning from batches\n",
        "        self.mask_teacher_batches(episodes, batch)\n",
        "        # Augment observations with identity\n",
        "        self.augment_critic_identity(batch, episodes)\n",
        "        # Loop through all modules and perform each one's GAE computation.\n",
        "        for module_id, module_vf_preds in vf_preds.items():\n",
        "            # Skip those outputs of RLModules that are not implementers of\n",
        "            # `ValueFunctionAPI`.\n",
        "            if module_vf_preds is None:\n",
        "                continue\n",
        "\n",
        "            module = rl_module[module_id]\n",
        "            device = module_vf_preds.device\n",
        "            # Convert to numpy for the upcoming GAE computations.\n",
        "            module_vf_preds = convert_to_numpy(module_vf_preds)\n",
        "\n",
        "            # Collect (single-agent) episode lengths for this particular module.\n",
        "            episode_lens = [\n",
        "                len(e) for e in sa_episodes_list if e.module_id in [None, module_id]\n",
        "            ]\n",
        "\n",
        "            # Remove all zero-padding again, if applicable, for the upcoming\n",
        "            # GAE computations.\n",
        "            module_vf_preds = unpad_data_if_necessary(episode_lens, module_vf_preds)\n",
        "            # Compute value targets.\n",
        "            module_value_targets = compute_value_targets(\n",
        "                values=module_vf_preds,\n",
        "                rewards=unpad_data_if_necessary(\n",
        "                    episode_lens,\n",
        "                    convert_to_numpy(batch[module_id][Columns.REWARDS]),\n",
        "                ),\n",
        "                terminateds=unpad_data_if_necessary(\n",
        "                    episode_lens,\n",
        "                    convert_to_numpy(batch[module_id][Columns.TERMINATEDS]),\n",
        "                ),\n",
        "                truncateds=unpad_data_if_necessary(\n",
        "                    episode_lens,\n",
        "                    convert_to_numpy(batch[module_id][Columns.TRUNCATEDS]),\n",
        "                ),\n",
        "                gamma=self.gamma,\n",
        "                lambda_=self.lambda_,\n",
        "            )\n",
        "            assert module_value_targets.shape[0] == sum(episode_lens)\n",
        "\n",
        "            module_advantages = module_value_targets - module_vf_preds\n",
        "            # Drop vf-preds, not needed in loss. Note that in the DefaultPPORLModule,\n",
        "            # vf-preds are recomputed with each `forward_train` call anyway to compute\n",
        "            # the vf loss.\n",
        "            # Standardize advantages (used for more stable and better weighted\n",
        "            # policy gradient computations).\n",
        "            module_advantages = (module_advantages - module_advantages.mean()) / max(\n",
        "                1e-4, module_advantages.std()\n",
        "            )\n",
        "            # Zero-pad the new computations, if necessary.\n",
        "            if module.is_stateful():\n",
        "                module_advantages = np.stack(\n",
        "                    split_and_zero_pad_n_episodes(\n",
        "                        module_advantages,\n",
        "                        episode_lens=episode_lens,\n",
        "                        max_seq_len=module.model_config[\"max_seq_len\"],\n",
        "                    ),\n",
        "                    axis=0,\n",
        "                )\n",
        "                module_value_targets = np.stack(\n",
        "                    split_and_zero_pad_n_episodes(\n",
        "                        module_value_targets,\n",
        "                        episode_lens=episode_lens,\n",
        "                        max_seq_len=module.model_config[\"max_seq_len\"],\n",
        "                    ),\n",
        "                    axis=0,\n",
        "                )\n",
        "            batch[module_id][Postprocessing.ADVANTAGES] = module_advantages\n",
        "            batch[module_id][Postprocessing.VALUE_TARGETS] = module_value_targets\n",
        "\n",
        "        # Convert all GAE results to tensors.\n",
        "        if self._numpy_to_tensor_connector is None:\n",
        "            self._numpy_to_tensor_connector = NumpyToTensor(\n",
        "                as_learner_connector=True, device=device\n",
        "            )\n",
        "        tensor_results = self._numpy_to_tensor_connector(\n",
        "            rl_module=rl_module,\n",
        "            batch={\n",
        "                mid: {\n",
        "                    Postprocessing.ADVANTAGES: module_batch[Postprocessing.ADVANTAGES],\n",
        "                    Postprocessing.VALUE_TARGETS: (\n",
        "                        module_batch[Postprocessing.VALUE_TARGETS]\n",
        "                    ),\n",
        "                }\n",
        "                for mid, module_batch in batch.items()\n",
        "                if vf_preds[mid] is not None\n",
        "            },\n",
        "            episodes=episodes,\n",
        "        )\n",
        "        # Move converted tensors back to `batch`.\n",
        "        for mid, module_batch in tensor_results.items():\n",
        "            batch[mid].update(module_batch)\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZFSdsuATekaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SharedCritic (no nn.Embedding)\n",
        "from typing import Dict\n",
        "\n",
        "from ray.rllib.algorithms.ppo.default_ppo_rl_module import DefaultPPORLModule\n",
        "from ray.rllib.algorithms.ppo.ppo_catalog import PPOCatalog\n",
        "from ray.rllib.core.columns import Columns\n",
        "from ray.rllib.core.models.base import ACTOR, CRITIC, ENCODER_OUT\n",
        "from ray.rllib.core.rl_module.apis.value_function_api import ValueFunctionAPI\n",
        "from ray.rllib.core.rl_module.rl_module import RLModule\n",
        "from ray.rllib.core.rl_module.torch import TorchRLModule\n",
        "from ray.rllib.utils.annotations import override\n",
        "from ray.rllib.utils.framework import try_import_torch\n",
        "from ray.rllib.utils.typing import TensorType\n",
        "from ray.util.annotations import DeveloperAPI\n",
        "\n",
        "torch, nn = try_import_torch()\n",
        "\n",
        "@DeveloperAPI\n",
        "class SharedCritic(TorchRLModule, ValueFunctionAPI):\n",
        "    def setup(self):\n",
        "        print(\"critic setup start\")\n",
        "        input_dim = self.observation_space[OBSERVATIONS].shape[0]\n",
        "        aug_dim = self.model_config[\"id_emb_size\"]\n",
        "        hidden_dim = self.model_config[\"hidden_dim\"]\n",
        "        self.vf = nn.Sequential( # Match default\n",
        "            nn.Linear(input_dim+aug_dim*2, hidden_dim),\n",
        "            torch.nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hidden_dim, 1),\n",
        "        )\n",
        "        self.emb = nn.Linear(300, aug_dim)\n",
        "\n",
        "    @override(ValueFunctionAPI)\n",
        "    def compute_values(self, batch: Dict[str, TensorType], embeddings=None):\n",
        "        # Check, if the observations are still in `dict` form.\n",
        "        if isinstance(batch[Columns.OBS], dict):\n",
        "            action_mask = batch[Columns.OBS].pop(ACTION_MASK)\n",
        "            batch[Columns.OBS] = batch[Columns.OBS].pop(OBSERVATIONS)\n",
        "            batch[ACTION_MASK] = action_mask\n",
        "        #\n",
        "        identity_ids = batch[AUGMENTATION]\n",
        "        one_hot = torch.nn.functional.one_hot(identity_ids, 300).float()\n",
        "        identity_emb = self.emb(one_hot)\n",
        "        identity_emb = identity_emb.reshape((identity_emb.shape[0],-1))\n",
        "        x = torch.cat((batch[Columns.OBS], identity_emb), dim=1)\n",
        "        #\n",
        "        vf_out = self.vf(x)\n",
        "        # Squeeze out last dimension (single node value head).\n",
        "        return vf_out.squeeze(-1)\n",
        "    def new_agent_embedding(self, ix_from, ix_to):\n",
        "        ''' Copy the embedding of a new agent from the embedding of its source.'''\n",
        "        with torch.no_grad():\n",
        "            self.emb.weight[:, ix_to] = self.emb.weight[:, ix_from].clone()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "K97xORKpWtfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IuO5KYusf-K"
      },
      "source": [
        "# Run training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X9gfH0MlsU1",
        "outputId": "6b1c27a9-8e3b-4324-bacd-4cf479a68142",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026-02-04 11:48:30,357\tWARNING algorithm_config.py:2480 -- DeprecationWarning: `config.training(learner_class=..)` has been deprecated. Use `config.learners(learner_class=..)` instead. This will raise an error in the future!\n",
            "2026-02-04 11:48:30,376\tWARNING algorithm_config.py:2503 -- DeprecationWarning: `config.training(learner_config_dict=..)` has been deprecated. Use `config.learners(learner_config_dict=..)` instead. This will raise an error in the future!\n",
            "2026-02-04 11:48:33,126\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "critic setup start\n"
          ]
        }
      ],
      "source": [
        "# @title config_league\n",
        "steps_to_save =  10 # number of epochs before saving a copy\n",
        "ptt = ['main','main_exploiter','league_exploiter']\n",
        "\n",
        "def atm_fn(agent_id, episode, **kwargs):\n",
        "    eid = hash(episode.id_)\n",
        "    rng = np.random.default_rng(seed=abs(eid))\n",
        "    student_agent, student_policy = get_learning_agent(\n",
        "        episode, ptt)\n",
        "    if (agent_id==student_agent):\n",
        "      return student_policy\n",
        "    # Select an opponent.\n",
        "    if (student_policy==\"main\"): # opponents for main\n",
        "      opponent = \"main\" if (rng.random() < .5) else rng.choice([\"main\",\"main_exploiter\",\"league_exploiter\",\"main_v0\"])\n",
        "      return opponent\n",
        "    elif (student_policy==\"main_exploiter\"): # opponents for ME\n",
        "      opponent = \"main\" if (rng.random() > .5) else \"main_v0\"\n",
        "      return opponent\n",
        "    else: # opponents for LE (all past players; fig 1)\n",
        "      opponent = \"main_v0\"\n",
        "      return opponent\n",
        "\n",
        "sp_callback = functools.partial(SelfPlayCallback,\n",
        "                                clone_every=steps_to_save)\n",
        "\n",
        "# The shared critic\n",
        "single_agent_env = TicTacToe()\n",
        "specs = {SHARED_CRITIC_ID: RLModuleSpec(\n",
        "    module_class=SharedCritic,\n",
        "    observation_space=single_agent_env.observation_spaces['X'],\n",
        "    action_space=single_agent_env.action_spaces['X'],\n",
        "    learner_only=True, # Only build on learner\n",
        "    model_config={\n",
        "        \"hidden_dim\": 256,\n",
        "        \"id_emb_size\": 32,\n",
        "    },\n",
        ")\n",
        "}\n",
        "\n",
        "for n in ptt+['main_v0']:\n",
        "    p = n\n",
        "    specs[p] = RLModuleSpec(\n",
        "        module_class=ActionMaskingTorchRLModule,\n",
        "        model_config={\n",
        "            \"head_fcnet_hiddens\": (64,64),\n",
        "            'fcnet_activation': 'relu',\n",
        "        }\n",
        "    )\n",
        "\n",
        "config = (\n",
        "    CMAPPOConfig()\n",
        "    .environment(TicTacToe, env_config={})\n",
        "    .callbacks( # set up our league\n",
        "        sp_callback\n",
        "    )\n",
        "    .env_runners(\n",
        "        num_env_runners=0,\n",
        "        num_envs_per_env_runner=1,\n",
        "    )\n",
        "    .multi_agent(\n",
        "          policies=['main','main_v0','main_exploiter','league_exploiter',SHARED_CRITIC_ID],\n",
        "          policy_mapping_fn=atm_fn,\n",
        "          # Only the learned policy should be trained.\n",
        "          policies_to_train=ptt+[SHARED_CRITIC_ID],\n",
        "      )\n",
        "    .training(\n",
        "        learner_class=CMAPPOTorchLearner,\n",
        "        learner_config_dict={\"policies_to_train\": ptt},\n",
        "        lr=1e-4,\n",
        "        train_batch_size=16384,\n",
        "        minibatch_size=2048,\n",
        "\n",
        "    )\n",
        "    .rl_module(\n",
        "        rl_module_spec=MultiRLModuleSpec(\n",
        "            rl_module_specs=specs\n",
        "        ),\n",
        "    )\n",
        ")\n",
        "\n",
        "algo = config.build_algo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efQGY7_Tsr_S"
      },
      "outputs": [],
      "source": [
        "num_iters = 101\n",
        "\n",
        "for i in range(num_iters):\n",
        "  results = algo.train()\n",
        "  if ENV_RUNNER_RESULTS in results:\n",
        "      mean_return = results[ENV_RUNNER_RESULTS].get(\n",
        "          'agent_episode_returns_mean', np.nan\n",
        "      )\n",
        "      vf_loss = results['learners'][SHARED_CRITIC_ID]['vf_loss']\n",
        "      mean_return = [(k, f'{v:.2f}') for k, v in mean_return.items()]\n",
        "      print(f\"iter={i+1} VF loss={vf_loss:.2f} R={mean_return}\")\n",
        "  if (i%10==0):\n",
        "      eval_model(algo.env_runner.module['main'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize"
      ],
      "metadata": {
        "id": "VNV9Pdj9ZHIz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRbXdoQogjNb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "outputId": "a0b7787a-9d3d-4781-b3ef-64760b0ce384",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7ad474c16450>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title Visualize agent embeddings\n",
        "agent_list = algo.config.policies\n",
        "\n",
        "agent_list = list(filter(lambda x: '_v' in x, agent_list))\n",
        "agent_list = sorted(agent_list, key=lambda x: int(x.split('_v')[-1]))\n",
        "\n",
        "main_agents = list(filter(lambda x: 'main_v' in x, agent_list))\n",
        "league_exploiters = list(filter(lambda x: 'league_exploiter_v' in x, agent_list))\n",
        "main_exploiters = list(filter(lambda x: 'main_exploiter_v' in x, agent_list))\n",
        "\n",
        "embeddings = algo.learner_group._learner._module[SHARED_CRITIC_ID].emb.weight.detach().clone().T\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "X = np.array(embeddings)\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(X)\n",
        "proj = pca.transform(embeddings)\n",
        "\n",
        "# Remember that ME and LE don't have a v0, and Main's V0 wasn't initialized from Main (nor was its embedding)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(proj[:,0],proj[:,1], c='gray', alpha=0.1)\n",
        "# Main agents\n",
        "plt.scatter(proj[0:len(main_agents),0],proj[0:len(main_agents),1])\n",
        "plt.scatter(proj[1,0],proj[1,1], color='gold')\n",
        "# League Exploiters\n",
        "plt.scatter(proj[100,0],proj[100,1], color='orange')\n",
        "plt.scatter(proj[102:102+len(league_exploiters),0],proj[102:102+len(league_exploiters),1], color='orange')\n",
        "# Main Exploiters\n",
        "plt.scatter(proj[200,0],proj[200,1], color='green')\n",
        "plt.scatter(proj[202:202+len(main_exploiters),0],proj[202:202+len(main_exploiters),1], color='green')\n",
        "# Learned agents\n",
        "plt.scatter(proj[[0,100,200],0],proj[[0,100,200],1], alpha=0.4, color='red', s=250, marker='x')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualize win rates\n",
        "N = 200 # @param {\"type\":\"integer\",\"placeholder\":\"50\"}\n",
        "from torch.distributions import Categorical\n",
        "learned_agent = algo.env_runner.module['main']\n",
        "\n",
        "env = TicTacToe()\n",
        "def query_agent(obs, agent):\n",
        "  if (isinstance(agent, torch.nn.Module)): # Learned\n",
        "    obs[OBSERVATIONS] = torch.tensor(obs[OBSERVATIONS])\n",
        "    obs[ACTION_MASK] = torch.tensor(obs[ACTION_MASK])\n",
        "    logits = agent.forward_inference({Columns.OBS: obs})['action_dist_inputs']\n",
        "    probs = torch.nn.Softmax()(logits)\n",
        "    action = Categorical(probs).sample()\n",
        "    return action.item()\n",
        "  else:\n",
        "    action = agent.forward_inference({Columns.OBS: torch.tensor(obs[OBSERVATIONS]).unsqueeze(0)})['actions'][0]\n",
        "    return int(action)\n",
        "\n",
        "# Agent, opponent, agent is X/O, win/lose/draw\n",
        "win_rates = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0))))\n",
        "\n",
        "for a1i in range(len(agent_list)):\n",
        "  agent_name = agent_list[a1i]\n",
        "  agent = algo.env_runner.module[agent_name]\n",
        "  print(f\"Calculating win rates for {agent_name}\")\n",
        "  for a2i in range(a1i, len(agent_list)):\n",
        "    opponent_name = agent_list[a2i]\n",
        "    opponent = algo.env_runner.module[opponent_name]\n",
        "    for play_as, play_against in [('X','O'), ('O','X')]:\n",
        "      agents = {\n",
        "          play_as: agent,\n",
        "          play_against: opponent\n",
        "      }\n",
        "      wr_dict = win_rates[agent_name][opponent_name][play_as]\n",
        "      wr_dict_r = win_rates[opponent_name][agent_name][play_against]\n",
        "      for i in range(N):\n",
        "        obs, _ = env.reset()\n",
        "        done = False\n",
        "        rewards = defaultdict(lambda: 0)\n",
        "        while (not done):\n",
        "          a = {}\n",
        "          for k in obs.keys():\n",
        "            a[k] = query_agent(obs[k], agents[k])\n",
        "          obs, r, term, trunc, _ = env.step(a)\n",
        "          # Render\n",
        "          for k, r in r.items():\n",
        "            rewards[k] += r\n",
        "          done = term['__all__']\n",
        "        wr_dict[np.sign(rewards[play_as])] += 1\n",
        "        wr_dict_r[-np.sign(rewards[play_as])] += 1"
      ],
      "metadata": {
        "id": "hLd4-ePaZJHS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Generate visualization\n",
        "import matplotlib.pyplot as plt\n",
        "merge = True # @param {\"type\":\"boolean\"}\n",
        "show_draws = False # @param {\"type\":\"boolean\"}\n",
        "fontsize = 7 # @param {\"type\":\"integer\"}\n",
        "cell_size = 0.14 # @param {\"type\":\"number\"} # inches per cell\n",
        "left_label_space = 0.9\n",
        "right_margin = 0.2\n",
        "top_label_space = 0.6\n",
        "bottom_margin = 0.2\n",
        "\n",
        "# Agent, opponent, agent is X/O, win/lose/draw\n",
        "def get_color(wld):\n",
        "  if (wld[1]+wld[-1]==0):\n",
        "    return [1,1,1]\n",
        "  if (show_draws):\n",
        "    colors = np.array([[0,0,1],[1,1,1],[1,0,0]])\n",
        "    wlda = np.array([wld[1], wld[0], wld[-1]],dtype=float)\n",
        "    if (wlda[0]+wlda[2]>0):\n",
        "      wld_cap = 0.2\n",
        "      wld_m = 1. / (1.-wlda[1]/wlda.sum()*(1-wld_cap))\n",
        "      wlda[1] *= wld_cap\n",
        "      wlda[[0,2]] *= wld_m\n",
        "    wld = wlda\n",
        "  else:\n",
        "    colors = np.array([[0,0,1],[1,0,0]])\n",
        "    wld = np.array([wld[1], wld[-1]])\n",
        "  return (wld/wld.sum()) @ colors\n",
        "\n",
        "groups = [\n",
        "    (\"Main\", main_agents),\n",
        "    (\"League Exploiter\", league_exploiters),\n",
        "    (\"Main Exploiter\", main_exploiters),\n",
        "]\n",
        "\n",
        "def merge_data(wrab):\n",
        "  tmp = wrab['X'].copy()\n",
        "  for k in [1,0,-1]:\n",
        "    tmp[k] += wrab['O'][k]\n",
        "  return tmp\n",
        "\n",
        "def build_color_array(row_agents, col_agents):\n",
        "    rows, cols = len(row_agents), len(col_agents)\n",
        "    arr = np.zeros((rows, cols, 3), dtype=float)\n",
        "    for i, a in enumerate(row_agents):\n",
        "        for j, b in enumerate(col_agents):\n",
        "            if (merge):\n",
        "              data = merge_data(win_rates[a][b])\n",
        "            else:\n",
        "              data = win_rates[a][b]['X']\n",
        "            arr[i, j] = get_color(data)\n",
        "    return arr\n",
        "\n",
        "col_counts = [len(g[1]) for g in groups]\n",
        "row_counts = [len(g[1]) for g in groups]\n",
        "\n",
        "# figure size\n",
        "fig_w = sum(col_counts) * cell_size + left_label_space + right_margin + 0.2  # extra margin for spacing\n",
        "fig_h = sum(row_counts) * cell_size + top_label_space + bottom_margin + 0.2\n",
        "\n",
        "left_frac = left_label_space / fig_w\n",
        "right_frac = 1.0 - (right_margin / fig_w)\n",
        "bottom_frac = bottom_margin / fig_h\n",
        "top_frac = 1.0 - (top_label_space / fig_h)\n",
        "\n",
        "fig = plt.figure(figsize=(fig_w, fig_h))\n",
        "\n",
        "# add spacing here:\n",
        "wspace = 0.2  # horizontal space between subplot blocks\n",
        "hspace = 0.2  # vertical space between subplot blocks\n",
        "\n",
        "gs = fig.add_gridspec(\n",
        "    3, 3,\n",
        "    width_ratios=col_counts,\n",
        "    height_ratios=row_counts,\n",
        "    left=left_frac, right=right_frac, bottom=bottom_frac, top=top_frac,\n",
        "    wspace=wspace, hspace=hspace\n",
        ")\n",
        "\n",
        "axes = np.empty((3, 3), dtype=object)\n",
        "for i, (row_name, row_group) in enumerate(groups):\n",
        "    for j, (col_name, col_group) in enumerate(groups):\n",
        "        ax = fig.add_subplot(gs[i, j])\n",
        "        axes[i, j] = ax\n",
        "        col_arr = build_color_array(row_group, col_group)\n",
        "        if col_arr.size == 0:\n",
        "            ax.axis(\"off\")\n",
        "            continue\n",
        "        ax.imshow(col_arr, interpolation=\"nearest\", aspect=\"equal\", origin=\"upper\")\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.set_frame_on(False)\n",
        "\n",
        "# column group names\n",
        "for j, (col_name, _) in enumerate(groups):\n",
        "    pos = axes[0, j].get_position()\n",
        "    x_center = pos.x0 + pos.width / 2.0\n",
        "    y_top = pos.y1 + 0.01\n",
        "    if (not merge):\n",
        "      col_name += ' (O)'\n",
        "    fig.text(x_center, y_top, col_name, ha=\"center\", va=\"bottom\", fontsize=fontsize)\n",
        "\n",
        "# row group names\n",
        "for i, (row_name, _) in enumerate(groups):\n",
        "    pos = axes[i, 0].get_position()\n",
        "    y_center = pos.y0 + pos.height / 2.0\n",
        "    x_left = pos.x0 - 0.01\n",
        "    if (not merge):\n",
        "      row_name += ' (X)'\n",
        "    fig.text(x_left, y_center, row_name, ha=\"right\", va=\"center\", fontsize=fontsize, rotation=\"vertical\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp1SiknO58Kv",
        "outputId": "19a48d30-c5d7-4e57-872f-9c756ca131da",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 494x464 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4RZXX2_4X6xJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}